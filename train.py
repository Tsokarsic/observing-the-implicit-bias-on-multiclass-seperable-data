import torch
import torch.nn as nn
from transformers.integrations import run_hp_search_wandb

import wandb
import json
import os
import argparse
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from utils import *
import datetime

from typing import Dict, Any
# ==============================================================================
# 1. ä¸»è®­ç»ƒå‡½æ•°
# ==============================================================================

def train(config_path: str = "config.json",optimizer1=None):
    # 1. åŠ è½½é…ç½®ï¼šç›´æ¥å‘½åä¸º config
    with open(config_path, 'r') as f:
        config: Dict[str, Any] = json.load(f)
    use_wandb=config['use_wandb']
    # åŠ¨æ€ç”Ÿæˆ WandB è¿è¡Œåç§° (ä½¿ç”¨å­—å…¸è®¿é—® config['key'])
    now = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    if optimizer1 is not None:
        optimizer_name = optimizer1
        config['training']['optimizer']=optimizer1
    else:
        optimizer_name = config['training']['optimizer']
    print(optimizer_name)
    optim_params = config['training']['optimizer_params'].get(optimizer_name, {})
    base_lr = optim_params.get('lr', 1e-3)
    if "momentum" in optim_params:
        momentum = optim_params['momentum']
    else:
        momentum = 0
    run_name = f"{now}_{optimizer_name}_lr{base_lr}_momentum{momentum}"

    # 2. é…ç½® WandB
    # æ³¨æ„ï¼šä¸å†å°† wandb.config èµ‹å€¼ç»™ä»»ä½•å˜é‡
    if use_wandb:
        wandb.init(
            project=config['wandb_project'],
            config=config,  # ä¼ å…¥åŸå§‹å­—å…¸
            name=run_name
        )

    # 3. æ•°æ®åŠ è½½æˆ–ç”Ÿæˆ
    print("--- 1. æ•°æ®åŠ è½½ä¸ Max Margin æ±‚è§£ ---")
    try:
        # load_data_or_generate æ¥æ”¶é…ç½®çš„ 'data' å­å­—å…¸
        X_np, y_np, _, _, max_margin_results = load_data_or_generate(config_data=config['data'])
    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯ï¼šæ•°æ®å¤„ç†å¤±è´¥ã€‚")
        wandb.finish()
        raise e

    # 4. åˆå§‹åŒ–æ¨¡å‹ã€DataLoader
    X_tensor = torch.tensor(X_np, dtype=torch.float32)
    y_tensor = torch.tensor(y_np, dtype=torch.long)
    n_samples, d_features = X_tensor.shape
    # sigma=config['training']['noise_scale']
    dataset = TensorDataset(X_tensor, y_tensor)
    batch_size=config['training']['batch_size']
    if not int(batch_size) >=1 :
        dataloader = DataLoader(dataset, batch_size=n_samples, shuffle=False)
        max_epochs = config['training']['epochs']
        batch_size = n_samples
    else:
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        max_epochs = int(config['training']['epochs'] / n_samples * batch_size)
    init_method = config['training'].get('init_method', 'gaussian').lower()  # é»˜è®¤ä½¿ç”¨é«˜æ–¯
    init_scale = config['training'].get('init_scale', 0.01)
    model = nn.Linear(d_features, config['data']['k'], bias=False)
    loss_fn = nn.CrossEntropyLoss()
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # é›¶åˆå§‹åŒ– (Zero Initialization)
            if init_method == 'zero':
                nn.init.constant_(module.weight, 0.0)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)
                print(f"   -> æ¨¡å— '{name}' æƒé‡è®¾ä¸ºé›¶ã€‚")

            # é«˜æ–¯åˆå§‹åŒ– (Gaussian/Normal Initialization)
            elif init_method == 'gaussian':
                # ä½¿ç”¨ PyTorch å†…å»ºçš„ Normal åˆå§‹åŒ–
                nn.init.normal_(module.weight, mean=0.0, std=init_scale)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)  # åç½®é€šå¸¸è®¾ä¸ºé›¶
                print(f"   -> æ¨¡å— '{name}' æƒé‡ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒ (Std={init_scale})ã€‚")

            # é»˜è®¤/å…¶ä»–åˆå§‹åŒ–
            else:
                warnings.warn(f"âš ï¸ æœªçŸ¥çš„åˆå§‹åŒ–æ–¹æ³• '{init_method}'ã€‚ä½¿ç”¨ PyTorch é»˜è®¤åˆå§‹åŒ–ã€‚")
                # PyTorch é»˜è®¤åˆå§‹åŒ– (é€šå¸¸æ˜¯ Kaiming Uniform)

    # 5. åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    # ä¼ å…¥ config çš„ 'training' å­å­—å…¸
    optimizer = get_optimizer(model, config['training'])
    lr_scheduler = get_lr_scheduler(config['training'],optim_params)

    # 6. è®­ç»ƒå¾ªç¯
    print(f"\n--- 2. å¼€å§‹è®­ç»ƒ ---")
    current_step = 0

    print(max_epochs)
    # ä½¿ç”¨å­—å…¸è®¿é—® config['training']['epochs']
    for epoch in range(1,max_epochs + 1):
        for inputs, targets in dataloader:

            current_step += 1

            # ğŸš€ æ ¸å¿ƒï¼šè®¡ç®—å½“å‰å­¦ä¹ ç‡ (1/sqrt(t) è¡°å‡)
            current_lr = lr_scheduler(current_step)

            # ç»Ÿä¸€çš„ LR æ›´æ–°æœºåˆ¶
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, targets)
            loss.backward()
            # if sigma > 1e-8:  # é¿å…sigmaä¸º0æ—¶æ— æ„ä¹‰è®¡ç®—
            #     for param in model.parameters():
            #         if param.grad is not None:  # ç¡®ä¿æ¢¯åº¦å­˜åœ¨
            #             # ç”Ÿæˆä¸æ¢¯åº¦åŒå½¢çŠ¶çš„æ ‡å‡†é«˜æ–¯å™ªå£° (å‡å€¼0ï¼Œæ ‡å‡†å·®sigma)
            #             noise = torch.randn_like(param.grad) * sigma*torch.norm(param.grad,"fro")
            #             # å åŠ å™ªå£°åˆ°æ¢¯åº¦ä¸Š
            #             param.grad.add_(noise)
            optimizer.step()

        # 7. æŒ‡æ ‡è®¡ç®—ä¸æ—¥å¿—è®°å½•
        # ä½¿ç”¨å­—å…¸è®¿é—® config['training']['log_interval']
        if current_step % config['training']['log_interval'] == 0 or epoch == max_epochs:
            Wt = model.weight.data
            metrics = calculate_implicit_bias_metrics(Wt, X_tensor, y_tensor, max_margin_results)

            # å…¨å±€å‡†ç¡®ç‡ï¼šåœ¨å½“å‰æ¨¡å‹æƒé‡ä¸‹ç”¨å…¨é‡æ ·æœ¬è¯„ä¼°
            with torch.no_grad():
                all_outputs = model(X_tensor)
                _, predicted = torch.max(all_outputs, 1)
                correct = (predicted == y_tensor).sum().item()
                # print(predicted)
                accuracy = correct / n_samples

            # --- æå–æ‰“å°æ‰€éœ€æ•°æ® (Current/Optimal Gamma & Correlation) ---

            # L2 Norm (Frobenius)
            normalized_L2_gamma = metrics['gamma_norm/L2_norm_normalized_gamma']
            optimal_L2_gamma = max_margin_results['L2_norm']['gamma']
            L2_corr = metrics['corr/L2_norm_correlation']

            # Linf Norm
            normalized_Linf_gamma = metrics['gamma_norm/Linf_norm_normalized_gamma']
            optimal_Linf_gamma = max_margin_results['Linf_norm']['gamma']
            Linf_corr = metrics['corr/Linf_norm_correlation']

            # Spectral Norm
            normalized_spec_gamma = metrics['gamma_norm/spectral_norm_normalized_gamma']
            optimal_spec_gamma = max_margin_results['spectral_norm']['gamma']
            spec_corr = metrics['corr/spectral_norm_correlation']

            normalized_nuclear_gamma = metrics['gamma_norm/nuclear_norm_normalized_gamma']
            optimal_nuclear_gamma = max_margin_results['nuclear_norm']['gamma']
            nuclear_corr = metrics['corr/nuclear_norm_correlation']

            log_data = {
                "step": current_step,
                "loss/train_loss": loss.item(),
                "accuracy/train_accuracy": accuracy,
                "lr/current_lr": current_lr,
                **metrics
            }
            if use_wandb:
                wandb.log(log_data, step=epoch)

            # --- æœ€ç»ˆä¿®æ”¹åçš„ Print è¯­å¥ ---
            print(
                f"Step {current_step}/{config['training']['epochs']} | LRï¼š{current_lr}.4f ï½œ Loss: {loss.item():.6f} | Acc: {accuracy:.4f} | "
                f"G(L2): {normalized_L2_gamma:.4f}/{optimal_L2_gamma:.4f} (Corr: {L2_corr:.4f}) | "
                f"G(Linf): {normalized_Linf_gamma:.4f}/{optimal_Linf_gamma:.4f} (Corr: {Linf_corr:.4f}) | "
                f"G(Spec): {normalized_spec_gamma:.4f}/{optimal_spec_gamma:.4f} (Corr: {spec_corr:.4f}) | "
                f"G(Nuclear): {normalized_nuclear_gamma:.4f}/{optimal_nuclear_gamma:.4f} (Corr: {nuclear_corr:.4f})"
            )
        if epoch == max_epochs:
            matrix = model.weight.data
            matrix=matrix/np.linalg.norm(matrix)
            # Plot singular values

    print("\n--- 3. è®­ç»ƒå®Œæˆ ---")
    if use_wandb:
        wandb.finish()
    return matrix
train(config_path="config.json")
matrix1=train(config_path="config.json",optimizer1="Muon")
matrix2=train(config_path="config.json",optimizer1="NucGD")
matrix3=train(config_path="config.json",optimizer1="NGD")
matrix4=train(config_path="config.json",optimizer1="SignGD")
import matplotlib.pyplot as plt
weight_matrices={"Muon(Lspec)":matrix1,"NucGD(Lnuc)":matrix2,"NGD(L2)":matrix3,"SignGD(Linf)":matrix4}
for name, matrix in weight_matrices.items():
    # Compute singular values
    _, s, _ = np.linalg.svd(matrix)
    # Plot singular values
    plt.semilogy(range(1, len(s)+1), s, 'o-', label=name)
plt.xlabel('Index')
plt.ylabel('Singular Value (log scale)')
plt.title('Singular Value Spectrum of Solutions Of NSD Under Different Norm')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig("spectrum_for_algorithms.png")
plt.show()
