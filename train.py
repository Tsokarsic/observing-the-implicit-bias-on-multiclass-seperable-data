import torch
import torch.nn as nn
import wandb
import json
import os
import argparse
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from utils import *
from generate_data import *
import datetime

from typing import Dict, Any
# ==============================================================================
# 1. ä¸»è®­ç»ƒå‡½æ•°
# ==============================================================================

def train(config_path: str = "config.json"):
    # 1. åŠ è½½é…ç½®ï¼šç›´æ¥å‘½åä¸º config
    with open(config_path, 'r') as f:
        config: Dict[str, Any] = json.load(f)

    # åŠ¨æ€ç”Ÿæˆ WandB è¿è¡Œåç§° (ä½¿ç”¨å­—å…¸è®¿é—® config['key'])
    now = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    optimizer_name = config['training']['optimizer']
    optim_params = config['training']['optimizer_params'].get(optimizer_name, {})
    base_lr = optim_params.get('lr', 1e-3)
    run_name = f"{now}_{optimizer_name}_lr{base_lr}"

    # 2. é…ç½® WandB
    # æ³¨æ„ï¼šä¸å†å°† wandb.config èµ‹å€¼ç»™ä»»ä½•å˜é‡
    wandb.init(
        project=config['wandb_project'],
        config=config,  # ä¼ å…¥åŸå§‹å­—å…¸
        name=run_name
    )

    # 3. æ•°æ®åŠ è½½æˆ–ç”Ÿæˆ
    print("--- 1. æ•°æ®åŠ è½½ä¸ Max Margin æ±‚è§£ ---")
    try:
        # load_data_or_generate æ¥æ”¶é…ç½®çš„ 'data' å­å­—å…¸
        X_np, y_np, _, _, max_margin_results = load_data_or_generate(config_data=config['data'])
    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯ï¼šæ•°æ®å¤„ç†å¤±è´¥ã€‚")
        wandb.finish()
        raise e

    # 4. åˆå§‹åŒ–æ¨¡å‹ã€DataLoader
    X_tensor = torch.tensor(X_np, dtype=torch.float32)
    y_tensor = torch.tensor(y_np, dtype=torch.long)
    n_samples, d_features = X_tensor.shape

    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=n_samples, shuffle=False)

    # ä¿®æ­£ï¼šä½¿ç”¨å­—å…¸è®¿é—® config['data']['k']
    model = nn.Linear(d_features, config['data']['k'], bias=False)
    loss_fn = nn.CrossEntropyLoss()

    # 5. åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    # ä¼ å…¥ config çš„ 'training' å­å­—å…¸
    optimizer = get_optimizer(model, config['training'])
    lr_scheduler = get_lr_scheduler(config['training'])

    # 6. è®­ç»ƒå¾ªç¯
    print(f"\n--- 2. å¼€å§‹è®­ç»ƒ ---")
    current_step = 0

    # ä½¿ç”¨å­—å…¸è®¿é—® config['training']['epochs']
    for epoch in range(1, config['training']['epochs'] + 1):
        for inputs, targets in dataloader:

            current_step += 1

            # ğŸš€ æ ¸å¿ƒï¼šè®¡ç®—å½“å‰å­¦ä¹ ç‡ (1/sqrt(t) è¡°å‡)
            current_lr = lr_scheduler(current_step)

            # ç»Ÿä¸€çš„ LR æ›´æ–°æœºåˆ¶
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, targets)
            loss.backward()

            optimizer.step()

        # 7. æŒ‡æ ‡è®¡ç®—ä¸æ—¥å¿—è®°å½•
        # ä½¿ç”¨å­—å…¸è®¿é—® config['training']['log_interval']
        if epoch % config['training']['log_interval'] == 0 or epoch == config['training']['epochs']:
            Wt = model.weight.data
            metrics = calculate_implicit_bias_metrics(Wt, X_tensor, y_tensor, max_margin_results)

            _, predicted = torch.max(outputs.data, 1)
            correct = (predicted == targets).sum().item()
            accuracy = correct / n_samples

            log_data = {
                "epoch": epoch,
                "loss/train_loss": loss.item(),
                "accuracy/train_accuracy": accuracy,
                "lr/current_lr": current_lr,
                **metrics
            }
            wandb.log(log_data, step=epoch)

            print(
                f"Epoch {epoch}/{config['training']['epochs']} | Loss: {loss.item():.6f} | LR: {current_lr:.6e} | Spec_Err: {metrics['gamma_error/spectral_norm_error_from_opt']:.4e}")

    print("\n--- 3. è®­ç»ƒå®Œæˆ ---")
    wandb.finish()

train(config_path="config.json")